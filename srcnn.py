# -*- coding: utf-8 -*-
"""SRCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qn_EaTgUCphkwJQg_3VPNptJgPIKaEJs

###Define a Convolutional Neural Network
"""

import numpy as np
import torch
from torch.autograd import Variable
from torch.nn.parameter import Parameter
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.nn.init as init
from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout2d
from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax

def compute_conv_dim(dim_size, kernel_size, padding, stride):
    return int((dim_size - kernel_size + 2 * padding) / stride + 1)
def compute_maxpool_dim(dim_size):
    return int( ( (dim_size + 2*maxpool_padding - maxpool_dilation*(maxpool_kernel1-1) - 1 )/maxpool_stride) + 1)

# Typical and basic setting according to the paper:
# f_1 = 9, f_2 = 1, f_3 = 5, n_1 = 64, n_2 = 32

channels = 3 #trainset.data.shape[3] #change accordingly to the input
height = 224 #trainset.data.shape[1] # change accordingly
width = 224 #trainset.data.shape[2] # change accordingly
stride = 1 # [stride_height, stride_width]

n_1 = 64 # number of filters
f_1 = 9  #kernel_size_conv1 = 5 # [height, width]
padding_1 = int(np.floor(f_1/2)) # to keep the same pixel size of the image (stride is 1)

n_2 = 32
f_2 = 1
padding_2 = int(np.floor(f_2/2))

f_3 = 5
padding_3 = int(np.floor(f_3/2))

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        
        # Patch extraction and representation layer
        self.conv_1   = Conv2d(in_channels=channels,
                               out_channels=n_1,
                               kernel_size=f_1,
                               stride=stride,
                               padding=padding_1)
        self.conv1_out_height = compute_conv_dim(height, f_1, padding_1, stride)
        self.conv1_out_width = compute_conv_dim(width, f_1, padding_1, stride)

        # Non-linear mapping layer 
        self.conv_2   = Conv2d(in_channels=n_1,
                               out_channels=n_2,
                               kernel_size=f_2,
                               stride=stride,
                               padding=padding_2)
        
        self.conv2_out_height = compute_conv_dim(self.conv1_out_height, f_2, padding_2, stride)
        self.conv2_out_width = compute_conv_dim(self.conv1_out_width, f_2, padding_2, stride)


        # Reconstruction layer
        self.conv_3   = Conv2d(in_channels=n_2,
                               out_channels=channels,
                               kernel_size=f_3,
                               stride=stride,
                               padding=padding_3)
        
        # self.conv3_out_height = compute_conv_dim(self.conv2_out_height, f_3, padding_3, stride)
        # self.conv3_out_width = compute_conv_dim(self.conv2_out_width, f_3, padding_3, stride)

    def forward(self, x):
        x = relu(self.conv_1(x))
        x = self.conv_2(x)
        x = self.conv_3(x)
        return x 

net = Net()
if torch.cuda.is_available():
    print('##converting network to cuda-enabled')
    net.cuda()
print(net)

"""### Define a loss function and a optimizer"""

import torch.optim as optim

criterion = nn.MSELoss()
optimizer = optim.SGD(net.parameters(), lr=0.05)

#Test the forward pass with dummy data
x = np.random.normal(0,1, (5, 3, 32, 32)).astype('float32')
x = Variable(torch.from_numpy(x))
x = x.cuda()
output = net(x)
print([x.size() for x in output])

def get_variable(x):
    """ Converts tensors to cuda, if available. """
    if torch.cuda.is_available():
        return x.cuda()
    return x

def get_numpy(x):
    """ Get numpy array for both cuda and not. """
    if torch.cuda.is_available():
        return x.cpu().data.numpy()
    return x.data.numpy()

num_epoch = 20  # Your code here!


for epoch in range(num_epoch):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs
        inputs, labels = data

        # wrap them in Variable
        inputs, labels = get_variable(Variable(inputs)), get_variable(Variable(labels))

        # zero the parameter gradients
        # Your code here!
        optimizer.zero_grad()

        # forward + backward + optimize
        # Your code here!
        output = net(inputs)
        losses = criterion(output, labels)
        losses.backward()
        optimizer.step()

        running_loss += losses
        if i % 1000 == 999:    # print every 1000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 1000))
            running_loss = 0.0

print('Finished Training')

